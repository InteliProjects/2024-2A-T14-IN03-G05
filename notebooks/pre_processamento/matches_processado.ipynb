{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKqBnAbY_yFy"
      },
      "source": [
        "# Pré-Processamento de dados do Arquivo Matches\n",
        "\n",
        "O pré-processamento dos dados é uma etapa fundamental na construção de modelos preditivos eficazes. Ele garante que os dados estejam em uma forma adequada para análise, eliminando inconsistências, ajustando escalas e transformando variáveis para que possam ser utilizadas de forma eficiente pelos algoritmos de aprendizado de máquina.\n",
        "\n",
        "Este notebook é dedicado ao pré-processamento da tabela **Matches** fornecida pela IBM, que contém dados detalhados sobre partidas realizadas, incluindo informações sobre os times e seus respectivos resultados. O objetivo deste notebook é facilitar a visualização e a análise desses dados, proporcionando estatísticas essenciais para estudos e pesquisas.\n",
        "\n",
        "## Objetivo do Pré-Processamento de Dados\n",
        "\n",
        "O principal objetivo desta seção é preparar os dados para análises subsequentes, garantindo que eles estejam completos, consistentes e prontos para modelagem. As etapas a seguir abordam o tratamento de valores ausentes, remoção de colunas irrelevantes, tratamento de outliers e transformação de variáveis.\n",
        "\n",
        "## Como Usar Este Notebook\n",
        "\n",
        "1. **Configuração do Ambiente**:\n",
        "   - **Google Colab**: No Google Colab, será necessário fazer o upload das tabelas para o Google Drive e montar o drive no notebook.\n",
        "   - **Localmente**: Se for rodar o notebook localmente, é necessário baixar as tabelas e colocá-las no mesmo diretório do notebook ou ajustar os caminhos dos arquivos conforme necessário.\n",
        "\n",
        "2. **Instalação de Dependências**:\n",
        "   - Certifique-se de que todas as bibliotecas necessárias estão instaladas. Você pode instalar as dependências utilizando o seguinte comando:\n",
        "     ```python\n",
        "     !pip install -r requirements.txt\n",
        "     ```\n",
        "\n",
        "3. **Execução do Notebook**:\n",
        "   - Siga as células de código sequencialmente para garantir que todas as etapas sejam executadas corretamente. Cada seção do notebook está organizada para facilitar a compreensão e a análise dos dados.\n",
        "\n",
        "## Nesse Notebook Será Abordado\n",
        "\n",
        "1. **Pré-processamento de Dados**:\n",
        "   - Limpeza dos dados, incluindo o tratamento de valores ausentes e a remoção de duplicatas.\n",
        "   - Normalização e padronização dos dados utilizando técnicas como `StandardScaler` e `MinMaxScaler`.\n",
        "   - Codificação de variáveis categóricas utilizando `LabelEncoder`.\n",
        "\n",
        "   \n",
        "## Etapas do Pré-Processamento\n",
        "\n",
        "1. **Tratamento de Valores Ausentes**:\n",
        "   - A identificação de valores ausentes (missing values) é essencial para garantir a integridade dos dados. Nesta etapa, serão identificadas colunas com valores faltantes e os métodos de imputação ou remoção desses valores serão definidos. Dependendo do contexto, os valores ausentes podem ser preenchidos com a média, mediana, ou descartados.\n",
        "   \n",
        "2. **Exclusão de Colunas**:\n",
        "   - Algumas colunas podem não ser relevantes para a análise ou podem conter muitos valores ausentes. Neste caso, a exclusão dessas colunas se torna uma opção importante para garantir a qualidade do dataset e evitar viés nas análises.\n",
        "\n",
        "3. **Identificação de Outliers por Gráficos**:\n",
        "   - A visualização de dados por meio de gráficos, como boxplots, permite identificar a presença de outliers de forma visual. Isso ajuda a entender a distribuição das variáveis e identificar valores que se desviam significativamente do padrão esperado.\n",
        "\n",
        "4. **Identificação de Outliers Usando o IQR**:\n",
        "   - O método do intervalo interquartil (IQR) será utilizado para identificar outliers de maneira sistemática. Valores que estejam além de 1.5 vezes o IQR, abaixo do primeiro quartil ou acima do terceiro quartil, serão considerados outliers e tratados adequadamente.\n",
        "\n",
        "5. **Tratamento de Outliers por Winsorização**:\n",
        "   - Em vez de excluir outliers, que podem conter informações valiosas, utilizaremos a técnica de **winsorização**. Ela limita os valores extremos dentro de um intervalo aceitável, substituindo-os por valores próximos ao limite superior ou inferior da distribuição, preservando assim a integridade do dataset sem remover dados.\n",
        "\n",
        "6. **Codificação de Variáveis Categóricas**:\n",
        "   - As variáveis categóricas, como o nome dos times ou outras características qualitativas, serão transformadas em uma forma numérica para que possam ser utilizadas por algoritmos de aprendizado de máquina. A técnica de **One-Hot Encoding** ou **Label Encoding** será utilizada para esse processo, dependendo do número de categorias e da necessidade do modelo.\n",
        "\n",
        "7. **Normalização dos Dados**:\n",
        "   - Para garantir que todas as variáveis numéricas estejam na mesma escala, aplicaremos a normalização ou padronização dos dados. Isso é particularmente importante para algoritmos que são sensíveis à escala das variáveis, como métodos baseados em distância (KNN, SVM, etc.). As técnicas de **MinMaxScaler** ou **StandardScaler** serão aplicadas conforme a necessidade da modelagem.\n",
        "\n",
        "## Resumo das Etapas\n",
        "\n",
        "Após o pré-processamento, os dados estarão limpos e prontos para modelagem. O tratamento de valores ausentes, a remoção ou ajuste de outliers e a transformação de variáveis categóricas garantem que os dados estejam em uma condição ideal para algoritmos de aprendizado de máquina. A normalização das variáveis numéricas assegura que as variáveis estejam na mesma escala, o que é fundamental para melhorar o desempenho dos modelos preditivos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijIwb3AL_yF1"
      },
      "source": [
        "# Dependências\n",
        "Para rodar o notebook de forma local, é recomendado que inicie uma venv (ambiente virtual) e instale as dependências.\n",
        "\n",
        "Se estiver utilizando o Google Colab, pule esta etapa."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "377ADtO50l49"
      },
      "source": [
        "# Importando bibliotecas\n",
        "\n",
        "Aqui é importado as dependências necessárias para a executação do projeto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8Wx5s42OQqMr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd #para ler, visualizar e printar infos do df\n",
        "import matplotlib.pyplot as plt #para construir e customizar gráficos\n",
        "import seaborn as sns #para visualizar uns gráficos\n",
        "import numpy as np #numpy porque é sempre bom importar numpy\n",
        "import math #para executar operações matemáticas\n",
        "from scipy.stats.mstats import winsorize #para realizar a winsorização\n",
        "from sklearn.preprocessing import LabelEncoder #para realizar o pré-processamento de dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUULBGIl0pyp"
      },
      "source": [
        "# Carregando o dataset\n",
        "\n",
        "Feita a importação do arquivo para leitura dos dados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "al14FkqhuuTm"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('../../notebooks/data/matches_nao_tratado.csv', sep=',')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFPGYRDvcSLj"
      },
      "source": [
        "## 1.1 Missing Values\n",
        "\n",
        "No processo de pré-processamento de dados, uma das primeiras etapas envolve a verificação e o tratamento de valores ausentes, conhecidos como **missing values**. Valores ausentes podem surgir por diversos motivos e, se não forem tratados, podem comprometer a qualidade das análises e modelos preditivos. Entretanto, ao inspecionar o dataset utilizado neste projeto, foi constatado que ele não contém valores nulos ou errados.\n",
        "\n",
        "Como não foram encontrados valores nulos ou inconsistentes no dataset, não foi necessário aplicar técnicas de imputação ou remoção de dados. Este resultado é ideal, pois garante que todos os dados disponíveis podem ser utilizados diretamente nas análises e na construção dos modelos preditivos, sem a necessidade de intervenção adicional.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QksVsbS91L9G"
      },
      "source": [
        "## 1.2 Exclusão de linhas com status incompleto\n",
        "\n",
        "Durante o processo de pré-processamento, identificamos que algumas partidas no dataset possuem os status \"incomplete\" ou \"suspended\". Esses status indicam que as partidas não foram concluídas ou foram interrompidas, o que pode comprometer a qualidade das análises e previsões.\n",
        "\n",
        "Especificamente, observamos a seguinte distribuição de status no dataset:\n",
        "- 282 partidas com status \"incomplete\"\n",
        "- 93 partidas com status \"complete\"\n",
        "- 5 partidas com status \"suspended\"\n",
        "\n",
        "Para garantir a integridade dos dados e a precisão do modelo preditivo, decidimos remover todas as linhas que não possuíam o status \"complete\". Dessa forma, asseguramos que o dataset final seja composto apenas por partidas totalmente concluídas, fornecendo uma base mais confiável para as análises subsequentes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJ5CkEQU1PMm"
      },
      "outputs": [],
      "source": [
        "# Visualizar a contagem dos diferentes status\n",
        "print(df['status'].value_counts())\n",
        "\n",
        "# Filtrar as linhas onde o status não é \"incomplete\" nem \"suspended\"\n",
        "df_filtered = df[(df['status'] != 'incomplete') & (df['status'] != 'suspended')]\n",
        "\n",
        "# Verificar se as linhas foram removidas\n",
        "print(df_filtered['status'].value_counts())\n",
        "\n",
        "df_filtered.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RiGbeVc1Rmi"
      },
      "source": [
        "## 1.3 Exclusão de colunas\n",
        "\n",
        "Como parte do processo de pré-processamento, é importante identificar e remover colunas que não acrescentam valor significativo à análise ou que podem introduzir ruído nos modelos preditivos. No dataset original, identificamos várias colunas que foram consideradas desnecessárias para os objetivos do projeto, seja por serem redundantes, irrelevantes ou por não fornecerem informações úteis para as previsões.\n",
        "\n",
        "As colunas a seguir foram identificadas para exclusão com base nos seguintes critérios:\n",
        "- **Irrelevância para Análise Preditiva**: Colunas como `timestamp`, `date_GMT`, e `Game Week` foram removidas, pois não contribuem diretamente para a análise do desempenho das equipes ou para a previsão de resultados.\n",
        "- **Dados Redundantes**: Colunas que agregam informações já disponíveis em outras colunas, como `total_goals_at_half_time`, foram excluídas para evitar duplicidade.\n",
        "- **Colunas Incompletas ou de Difícil Metrificação**: Colunas como `referee` foram removidas por serem difíceis de quantificar de maneira que agregue valor ao modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdlJf8xJ1QQ9"
      },
      "outputs": [],
      "source": [
        "#Exclusão de colunas\n",
        "\n",
        "#Identificação de colunas a ser excluída\n",
        "colunas_dropar = ['timestamp', 'date_GMT', 'status', 'attendance', 'referee', 'Game Week', 'total_goal_count', 'total_goals_at_half_time', 'home_team_goal_count_half_time', 'away_team_goal_count_half_time', 'home_team_goal_timings', 'away_team_goal_timings', 'home_team_first_half_cards', 'home_team_second_half_cards', 'away_team_first_half_cards', 'away_team_second_half_cards', 'over_15_percentage_pre_match', 'over_25_percentage_pre_match', 'over_35_percentage_pre_match', 'over_45_percentage_pre_match', 'over_15_HT_FHG_percentage_pre_match', 'over_05_HT_FHG_percentage_pre_match', 'over_15_2HG_percentage_pre_match', 'over_05_2HG_percentage_pre_match', 'stadium_name']\n",
        "\n",
        "#Exclusão das colunas identificadas\n",
        "df = df_filtered.drop(columns=colunas_dropar)\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsHnx7Av9SCe"
      },
      "source": [
        "## 1.4 Codificação de variáveis categóricas\n",
        "\n",
        "Nessa etapa foi feito a transformação das variáveis categóricas para um formato numérico que possa ser interpretado pelos algoritmos de aprendizado de máquina. Para este propósito, foi utilizado a técnica de codificação através do `LabelEncoder`, que converte categorias em valores inteiros, facilitando sua utilização nos modelos.\n",
        "\n",
        "#### Variáveis Categóricas Codificadas\n",
        "\n",
        "No dataset, os nomes dos times de futebol são variáveis categóricas que precisam ser convertidas em formato numérico. Isso permitirá que os modelos interpretem as equipes como entradas válidas durante o processo de treinamento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqI-sFy75J1a"
      },
      "outputs": [],
      "source": [
        "# Inicializando o LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Codificando os nomes dos times\n",
        "df['home_team_encoded'] = label_encoder.fit_transform(df['home_team_name'])\n",
        "df['away_team_encoded'] = label_encoder.fit_transform(df['away_team_name'])\n",
        "\n",
        "# Ordenando as colunas codificadas em ordem crescente\n",
        "df = df.sort_values(by=['home_team_encoded', 'away_team_encoded']).reset_index(drop=True)\n",
        "\n",
        "df.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxtOtOin1Xtb"
      },
      "source": [
        "## 1.5 Identificação de Outliers\n",
        "\n",
        "Essa subseção foi feita a identificação e tratamento de outliers. Outliers são valores que se distanciam significativamente do restante dos dados, podendo distorcer análises e influenciar negativamente o desempenho do modelos preditivos. Dessa forma, é necessário identificar esses valores anômalos para decidir se eles devem ser tratados ou removidos, garantindo que as análises e modelos subsequentes sejam precisos e confiáveis.\n",
        "\n",
        "Abaixo foi feita a visualização através de boxplots para identificar outliers nas colunas numéricas do dataset. Os boxplots são ferramentas visuais eficazes para detectar outliers, pois mostram a distribuição dos dados e destacam pontos que estão além dos limites esperados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDMpep5d1elW"
      },
      "outputs": [],
      "source": [
        "def plot_boxplots_with_outliers(df, max_plots_per_fig=10):\n",
        "    # filtrar colunas numéricas dos dados\n",
        "    numeric_cols = df.select_dtypes(include=['number']).columns\n",
        "    # caso não tenha\n",
        "    if len(numeric_cols) == 0:\n",
        "        raise ValueError(\"Nenhuma coluna numérica no DataFrame.\")\n",
        "    # número de figuras necessárias pra plotar as colunas\n",
        "    num_figures = math.ceil(len(numeric_cols) / max_plots_per_fig)\n",
        "    for fig_idx in range(num_figures):\n",
        "        start_idx = fig_idx * max_plots_per_fig\n",
        "        end_idx = start_idx + max_plots_per_fig\n",
        "        cols_to_plot = numeric_cols[start_idx:end_idx]\n",
        "        fig, axes = plt.subplots(nrows=len(cols_to_plot), ncols=1, figsize=(8, len(cols_to_plot) * 4))\n",
        "        # se tiver só uma (tabela de league) não gera lista\n",
        "        if len(cols_to_plot) == 1:\n",
        "            axes = [axes]\n",
        "        for ax, col in zip(axes, cols_to_plot):\n",
        "            try:\n",
        "                sns.boxplot(x=df[col], ax=ax)\n",
        "                ax.set_title(f'Boxplot de {col}')\n",
        "                ax.set_xlabel(col)\n",
        "            except ValueError as e:\n",
        "                print(f\"Erro ao criar boxplot para a coluna '{col}': {e}\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "# informações para diagnóstico\n",
        "print(df.info())\n",
        "# primeiras colunas do dataframe para ter certeza que foi carregado corretamente\n",
        "print(df.head())\n",
        "plot_boxplots_with_outliers(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Xv8KqvxU0En"
      },
      "source": [
        "## 1.5.1 Quantidade de outliers por coluna\n",
        "\n",
        "Após a identificação visual dos outliers utilizando boxplots, é importante quantificar esses outliers em cada coluna numérica. A quantificação dos outliers nos dá uma visão mais clara da extensão dos valores atípicos em nosso dataset, permitindo decisões informadas sobre como lidar com eles. Com isso garantindo a integridade dos modelos preditivos, já que um grande número de outliers pode distorcer os resultados e influenciar negativamente o desempenho dos algoritmos.\n",
        "\n",
        "Nesta subseção, foi utilizado o método do Intervalo Interquartil (IQR) para identificar e contar a quantidade de outliers presentes em cada coluna numérica do dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_6A6ekmU3xA"
      },
      "outputs": [],
      "source": [
        "for column in df.select_dtypes(include=['number']).columns:\n",
        "    def identificar_outliers_iqr(coluna):\n",
        "      Q1 = coluna.quantile(0.25)\n",
        "      Q3 = coluna.quantile(0.75)\n",
        "      IQR = Q3 - Q1\n",
        "      limite_inferior = Q1 - 1.5 * IQR\n",
        "      limite_superior = Q3 + 1.5 * IQR\n",
        "      return (coluna < limite_inferior) | (coluna > limite_superior)\n",
        "\n",
        "# Aplicar a função apenas nas colunas numéricas\n",
        "outliers = df.select_dtypes(include=[np.number]).apply(identificar_outliers_iqr)\n",
        "\n",
        "# Contar a quantidade de outliers por coluna\n",
        "quantidade_outliers = outliers.sum()\n",
        "\n",
        "# Imprimir o resultado\n",
        "print(quantidade_outliers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGo4xI9IeLPi"
      },
      "source": [
        "## 1.6 Tratamento de outliers\n",
        "\n",
        "Para reduzir o impacto desses outliers sem removê-los completamente, foi utilizado uma técnica chamada **Winsorização**. A Winsorização ajusta os valores dos outliers para que fiquem dentro de limites especificados, permitindo que os dados extremos sejam mantidos, mas com menos influência nas análises.\n",
        "\n",
        "A Winsorização é utilizada para reduzir a influência de outliers nos dados sem removê-los, substituindo os valores fora de um certo limite por valores próximos ao limite. Isso preserva a integridade dos dados enquanto minimiza o impacto dos outliers extremos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMGFsFOwe92t"
      },
      "outputs": [],
      "source": [
        "# Função para aplicar Winsorização\n",
        "def aplicar_winsorizacao(coluna, limite_inferior=0.05, limite_superior=0.05):\n",
        "    return winsorize(coluna, limits=(limite_inferior, limite_superior))\n",
        "\n",
        "# Aplicar a Winsorização nas colunas numéricas\n",
        "df_winsorizado = df.select_dtypes(include=[np.number]).apply(aplicar_winsorizacao)\n",
        "\n",
        "# Substituir as colunas originais pelas winsorizadas\n",
        "df[df_winsorizado.columns] = df_winsorizado\n",
        "\n",
        "df.to_csv('../../notebooks/data/tratado/matches_tratado.csv', index=False)\n",
        "\n",
        "# Verificação dos resultados\n",
        "print(\"Valores mínimos e máximos depois da Winsorização:\")\n",
        "print(df_winsorizado.min())\n",
        "print(df_winsorizado.max())\n",
        "\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a6M--fBqXgn"
      },
      "source": [
        "# 2. Referências\n",
        "\n",
        "Está é uma seção de referências com relação as bibliotecas que utilizamos ao longo deste arquivo\n",
        "\n",
        "NUMPY. NumPy Documentation. Disponível em: <https://numpy.org/doc/>.\n",
        "\n",
        "‌PANDAS. pandas documentation. Disponível em: <https://pandas.pydata.org/docs/>.\n",
        "\n",
        "MATPLOTLIB. Matplotlib: Python plotting — Matplotlib 3.3.4 documentation. Disponível em: <https://matplotlib.org/stable/index.html>.\n",
        "\n",
        "‌SEABORN. seaborn: statistical data visualization — seaborn 0.9.0 documentation. Disponível em: <https://seaborn.pydata.org/>.\n",
        "\n",
        "‌SciPy documentation — SciPy v1.8.1 Manual. Disponível em: <https://docs.scipy.org/doc/scipy/>.\n",
        "\n",
        "‌PYTHON SOFTWARE FOUNDATION. Math — Mathematical Functions — Python 3.8.3rc1 Documentation. Disponível em: <https://docs.python.org/3/library/math.html>.\n",
        "\n",
        "‌SCIKIT-LEARN. scikit-learn: machine learning in Python. Disponível em: <https://scikit-learn.org/stable/>.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
